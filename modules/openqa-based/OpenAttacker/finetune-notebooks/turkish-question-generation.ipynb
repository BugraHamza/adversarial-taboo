{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datasets import Dataset as D\n",
    "from tqdm import tqdm\n",
    "\n",
    "!git clone https://github.com/TQuad/turkish-nlp-qa-dataset"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-03-27T12:17:46.297046Z",
     "iopub.execute_input": "2022-03-27T12:17:46.297366Z",
     "iopub.status.idle": "2022-03-27T12:17:51.291393Z",
     "shell.execute_reply.started": "2022-03-27T12:17:46.297285Z",
     "shell.execute_reply": "2022-03-27T12:17:51.290448Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tquad2df(path):\n",
    "    df = pd.DataFrame(columns=['title', 'content', 'question'])\n",
    "    \n",
    "    dataset = D.from_json(path)['data'][0]\n",
    "    \n",
    "    for data in tqdm(dataset):\n",
    "        title = data['title']\n",
    "        for para in data['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qa in para['qas']:\n",
    "                question = qa['question']\n",
    "                \n",
    "                df_el = pd.DataFrame([{'title': title, \n",
    "                                       'content': context, \n",
    "                                       'question': question}])\n",
    "                df = pd.concat([df, df_el], ignore_index=True)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-27T12:17:51.447190Z",
     "iopub.execute_input": "2022-03-27T12:17:51.447520Z",
     "iopub.status.idle": "2022-03-27T12:17:51.455412Z",
     "shell.execute_reply.started": "2022-03-27T12:17:51.447487Z",
     "shell.execute_reply": "2022-03-27T12:17:51.454513Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, tokenizer, device='cpu'):\n",
    "        self.X = X\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        tokenized_el = self.tokenizer(self.X[i][0], padding='max_length', return_tensors='pt')\n",
    "        \n",
    "        for k, v in tokenized_el.items():\n",
    "            v_device = v.to(self.device)               \n",
    "            tokenized_el[k] = v_device.squeeze()\n",
    "        \n",
    "        return tokenized_el\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-27T12:17:52.520013Z",
     "iopub.execute_input": "2022-03-27T12:17:52.520351Z",
     "iopub.status.idle": "2022-03-27T12:17:52.531412Z",
     "shell.execute_reply.started": "2022-03-27T12:17:52.520311Z",
     "shell.execute_reply": "2022-03-27T12:17:52.529601Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_tokenizable(df, tokenizer):\n",
    "    ls = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        el_sent = f\"{tokenizer.bos_token}Paragraf: {row.content} Soru: {row.question}{tokenizer.eos_token}\"\n",
    "        tok_sent = tokenizer.tokenize(el_sent)\n",
    "        \n",
    "        if len(tok_sent) < tokenizer.model_max_length-1:\n",
    "            ls.append(el_sent)\n",
    "            \n",
    "    return pd.DataFrame(data={'concat_content': ls})"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-27T12:17:53.198397Z",
     "iopub.execute_input": "2022-03-27T12:17:53.198893Z",
     "iopub.status.idle": "2022-03-27T12:17:53.205563Z",
     "shell.execute_reply.started": "2022-03-27T12:17:53.198855Z",
     "shell.execute_reply": "2022-03-27T12:17:53.204650Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def finetune(model, dataloader, epochs, optimizer, device='cpu'):\n",
    "    losses = []\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader)\n",
    "        pbar.set_description(f\"Epoch #{epoch+1}\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            inputs = batch['input_ids']\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            del inputs\n",
    "            del outputs\n",
    "            \n",
    "            pbar.set_postfix(bce_loss=f\"{sum(losses)/len(losses)}\")\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "def test(model, dataloader, device='cpu'):\n",
    "    loss_sum = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, batch in enumerate(pbar):\n",
    "            inputs = batch['input_ids']\n",
    "            loss = model(inputs, labels=inputs).loss\n",
    "            \n",
    "            loss_sum += loss\n",
    "        \n",
    "    loss_sum = loss_sum / i\n",
    "    perplexity = math.exp(loss_sum)\n",
    "    \n",
    "    return perplexity\n",
    "    \n",
    "def calc_perplexity(model, tokenizer, sentence):\n",
    "    sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "    sent = tokenizer(sent, padding='max_length', return_tensors='pt')\n",
    "    sent = sent['input_ids']\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = model(sent, labels=sent).loss\n",
    "        perplexity = math.exp(loss)\n",
    "        \n",
    "        return perplexity\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-27T12:17:53.998183Z",
     "iopub.execute_input": "2022-03-27T12:17:53.998902Z",
     "iopub.status.idle": "2022-03-27T12:17:54.011982Z",
     "shell.execute_reply.started": "2022-03-27T12:17:53.998865Z",
     "shell.execute_reply": "2022-03-27T12:17:54.010943Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TRAIN_DIR = './turkish-nlp-qa-dataset/train-v0.1.json'\n",
    "DEV_DIR= './turkish-nlp-qa-dataset/dev-v0.1.json'\n",
    "MODEL_PATH = 'redrussianarmy/gpt2-turkish-cased'\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "THRESHOLD = 35\n",
    "EPOCHS = 1\n",
    "LR = 0.001\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN, force_download=True)\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', \n",
    "                       'sep_token': '<SEP>', 'pad_token':'<PAD>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "train_df = tquad2df(TRAIN_DIR)\n",
    "train_df = get_tokenizable(train_df, tokenizer)\n",
    "\n",
    "dev_df = tquad2df(DEV_DIR)\n",
    "dev_df = get_tokenizable(dev_df, tokenizer)\n",
    "\n",
    "train_set = CustomDataset(train_df.to_numpy(), tokenizer=tokenizer, device=DEVICE)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_set = CustomDataset(dev_df.to_numpy(), tokenizer=tokenizer, device=DEVICE)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "gpt_config = AutoConfig.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN, max_position_embeddings=MAX_LEN, force_download=True)\n",
    "gpt_model = AutoModelForCausalLM.from_config(gpt_config)\n",
    "gpt_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "opt = optim.AdamW(gpt_model.parameters(), lr=LR) \n",
    "gpt_model, losses = finetune(gpt_model, train_loader, epochs=EPOCHS, optimizer=opt, device=DEVICE)\n",
    "perplexity = test(gpt_model, test_loader, device=DEVICE)\n",
    "print(f\"Test Set Perplexity: {perplexity}\")\n",
    "\n",
    "torch.save(gpt_model, f'question_generation_model_{perplexity}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-27T13:00:40.861566Z",
     "iopub.execute_input": "2022-03-27T13:00:40.861852Z",
     "iopub.status.idle": "2022-03-27T13:11:52.028185Z",
     "shell.execute_reply.started": "2022-03-27T13:00:40.861820Z",
     "shell.execute_reply": "2022-03-27T13:11:52.027427Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  }
 ]
}
