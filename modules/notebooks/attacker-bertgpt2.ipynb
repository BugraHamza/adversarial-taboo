{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install evaluate\n!pip install sacrebleu",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-19T18:42:35.403745Z",
     "iopub.execute_input": "2023-03-19T18:42:35.404050Z",
     "iopub.status.idle": "2023-03-19T18:43:00.940787Z",
     "shell.execute_reply.started": "2023-03-19T18:42:35.404022Z",
     "shell.execute_reply": "2023-03-19T18:43:00.939055Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.4/81.4 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.11.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.12.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (23.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.1)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2023.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.21.6)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.2)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (5.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.11.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.7.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0mCollecting sacrebleu\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m118.9/118.9 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (4.9.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (1.21.6)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2021.11.10)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.7.0)\nInstalling collected packages: sacrebleu\nSuccessfully installed sacrebleu-2.3.1\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import math\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, EncoderDecoderModel, set_seed\nimport evaluate\n\nfrom tqdm import tqdm\n\nset_seed(42)\n\nbleu = evaluate.load('sacrebleu')",
   "metadata": {
    "id": "w8IHlnyX14Tn",
    "execution": {
     "iopub.status.busy": "2023-03-19T18:43:00.948752Z",
     "iopub.execute_input": "2023-03-19T18:43:00.949674Z",
     "iopub.status.idle": "2023-03-19T18:43:17.572904Z",
     "shell.execute_reply.started": "2023-03-19T18:43:00.949607Z",
     "shell.execute_reply": "2023-03-19T18:43:17.571933Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22ea2c3b3add40b78a8130d21b5992ee"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "class TData(Dataset):\n    def __init__(self, df, encoder_tokenizer, decoder_tokenizer, device='cpu'):\n        super(TData, self).__init__()\n\n        self.df = df\n        self.encoder_tokenizer = encoder_tokenizer\n        self.decoder_tokenizer = decoder_tokenizer\n        self.device= device\n\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n\n        start = row['answer_start'] - row.context.find(row.cloze) - 1\n        end = start + len(row['answer'])\n\n        cloze = row.cloze[:start] + self.encoder_tokenizer.mask_token + row.cloze[end:]\n        \n        encoder_inputs = self.encoder_tokenizer(cloze, padding='max_length', max_length=256,\n                                                truncation=True, return_tensors='pt')\n        question = self.decoder_tokenizer.bos_token + row['question'] + self.decoder_tokenizer.eos_token\n        decoder_outputs = self.decoder_tokenizer(question, padding='max_length', max_length=256,\n                                                 truncation=True, return_tensors=\"pt\")\n\n        encoder_inputs = {k: v[0].to(self.device) for k, v in encoder_inputs.items()}\n        decoder_outputs = {k: v[0].to(self.device) for k, v in decoder_outputs.items()}\n\n        return {'encoder_inputs': encoder_inputs, \n                'decoder_outputs': decoder_outputs}\n\n    def __len__(self):\n        return len(self.df)",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "id": "szatB93k14To",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-03-19T18:43:17.574528Z",
     "iopub.execute_input": "2023-03-19T18:43:17.575418Z",
     "iopub.status.idle": "2023-03-19T18:43:17.585088Z",
     "shell.execute_reply.started": "2023-03-19T18:43:17.575377Z",
     "shell.execute_reply": "2023-03-19T18:43:17.584020Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def train_step(model, train_loader, criterion, optimizer):\n    model.train()\n\n    losses, ppls, bleus = [], [], []\n    pbar = tqdm(train_loader)\n    for i, batch in enumerate(pbar):\n        encoder_inputs = batch['encoder_inputs']\n        decoder_outputs = batch['decoder_outputs']\n\n        optimizer.zero_grad()\n        h_state = model.encoder(**encoder_inputs).last_hidden_state\n        out = model.decoder(**decoder_outputs, encoder_hidden_states=h_state, encoder_attention_mask=encoder_inputs['attention_mask']).logits\n        loss = criterion(out[:, :-1].moveaxis(1, -1), decoder_outputs['input_ids'][:, 1:])\n        \n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n        ppls.append(math.exp(loss.item()))\n        \n        references = [[o] for o in decoder_tokenizer.batch_decode(decoder_outputs['input_ids'], skip_special_tokens=True)]\n        predictions = decoder_tokenizer.batch_decode(out.argmax(dim=-1), skip_special_tokens=True)\n        results = bleu.compute(predictions=predictions, references=references)\n        bleus.append(results['score'])\n\n        pbar.set_description(f'Batch {i+1}/{len(train_loader)}: Loss: {np.mean(losses):.4f} - Perplexity: {np.mean(ppls):.4f} - Bleu: {np.mean(bleus):.4f}')\n\n    return np.mean(losses), np.mean(ppls), np.mean(bleus)\n\n\ndef eval_step(model, val_loader, criterion):\n    model.eval()\n\n    losses, ppls, bleus = [], [], []\n    with torch.no_grad():\n        pbar = tqdm(val_loader)\n        for i, batch in enumerate(pbar):\n            encoder_inputs = batch['encoder_inputs']\n            decoder_outputs = batch['decoder_outputs']\n\n            h_state = model.encoder(**encoder_inputs).last_hidden_state\n            out = model.decoder(**decoder_outputs, encoder_hidden_states=h_state, encoder_attention_mask=encoder_inputs['attention_mask']).logits\n            loss = criterion(out[:, :-1].moveaxis(1, -1), decoder_outputs['input_ids'][:, 1:])\n            \n            losses.append(loss.item())\n            ppls.append(math.exp(loss.item()))\n            \n            references = [[o] for o in decoder_tokenizer.batch_decode(decoder_outputs['input_ids'], skip_special_tokens=True)]\n            predictions = decoder_tokenizer.batch_decode(out.argmax(dim=-1), skip_special_tokens=True)\n            results = bleu.compute(predictions=predictions, references=references)\n            bleus.append(results['score'])\n            \n            pbar.set_description(f'Batch {i+1}/{len(val_loader)}: Loss: {np.mean(losses):.4f} - Perplexity: {np.mean(ppls):.4f} - Bleu: {np.mean(bleus):.4f}')\n\n    return np.mean(losses), np.mean(ppls), np.mean(bleus)\n\n\ndef train(model, train_loader, criterion, optimizer, val_loader=None, device='cpu'):\n    train_losses, train_ppls, train_bleus = [], [], []\n    val_losses, val_ppls, val_bleus = [], [], []\n\n    for epoch in range(EPOCHS):\n        print(f'Epoch {epoch+1}/{EPOCHS}')\n\n        train_loss, train_ppl, train_bleu = train_step(model, train_loader, criterion, optimizer)\n        train_losses.append(train_loss)\n        train_ppls.append(train_ppl)\n        train_bleus.append(train_bleu)\n\n        if val_loader is not None:\n            val_loss, val_ppl, val_bleu = eval_step(model, val_loader, criterion)\n            val_losses.append(val_loss)\n            val_ppls.append(val_ppl)\n            val_bleus.append(val_bleu)\n        \n        test_case = val_df.iloc[0]\n        question = ask(model, test_case.answer, test_case.cloze, device='cuda')\n        print(f'GENERATED -> {question}')\n\n    return train_losses, train_ppls, train_bleus, val_losses, val_ppls, val_bleus\n\n@torch.no_grad()\ndef ask(model, answer, cloze, answer_start=None, device='cpu'):\n    if answer_start is None:\n        answer_start = cloze.find(answer)\n\n    answer_end = answer_start + len(answer)\n    \n    cloze = cloze[:answer_start] + encoder_tokenizer.mask_token + cloze[answer_end:]\n\n    encoder_inputs = encoder_tokenizer(cloze, return_tensors='pt', truncation=True).input_ids.to(device)\n\n    bad_word_ids = decoder_tokenizer(answer, add_special_tokens=False).input_ids\n    generated_ids = model.generate(encoder_inputs,\n                                        decoder_start_token_id=decoder_tokenizer.bos_token_id,\n                                        num_beams=5, max_length=50, do_sample=True,\n                                        top_k=50, top_p=0.95, early_stopping=True,\n                                        pad_token_id=decoder_tokenizer.eos_token_id,\n                                        #no_repeat_ngram_size=2,\n                                        bad_words_ids=[bad_word_ids],\n                                        num_return_sequences=1)\n    \n    return decoder_tokenizer.batch_decode(generated_ids)",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "id": "dfeasnWf14Tp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a21c6758-e2cf-479f-e24c-68ce701eb060",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-03-19T18:43:17.588762Z",
     "iopub.execute_input": "2023-03-19T18:43:17.589387Z",
     "iopub.status.idle": "2023-03-19T18:43:17.612693Z",
     "shell.execute_reply.started": "2023-03-19T18:43:17.589348Z",
     "shell.execute_reply": "2023-03-19T18:43:17.611525Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "TRAIN_DIR = '/kaggle/input/my-quad/my_quad.csv'\n\ntrainval_df = pd.read_csv(TRAIN_DIR)\n\ntrain_df = trainval_df.sample(frac=.85, random_state=42)\nval_df = trainval_df.drop(train_df.index)",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "id": "oO-VZWdQ14Tq",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-03-19T18:43:17.615881Z",
     "iopub.execute_input": "2023-03-19T18:43:17.616175Z",
     "iopub.status.idle": "2023-03-19T18:43:17.685315Z",
     "shell.execute_reply.started": "2023-03-19T18:43:17.616149Z",
     "shell.execute_reply": "2023-03-19T18:43:17.684384Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "BATCH_SIZE = 2\nEPOCHS = 10\nDEVICE = 'cuda'",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "id": "D5HVYRr414Tq",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-03-19T18:43:17.686657Z",
     "iopub.execute_input": "2023-03-19T18:43:17.687046Z",
     "iopub.status.idle": "2023-03-19T18:43:17.694075Z",
     "shell.execute_reply.started": "2023-03-19T18:43:17.686997Z",
     "shell.execute_reply": "2023-03-19T18:43:17.692982Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "encoder_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\ndecoder_tokenizer = AutoTokenizer.from_pretrained(\"redrussianarmy/gpt2-turkish-cased\")\ndecoder_tokenizer.add_special_tokens({'bos_token': '<BOS>', 'pad_token': '<PAD>', 'eos_token': '<EOS>'})\n\nencoder_model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\").to(DEVICE)\ndecoder_model = AutoModelForCausalLM.from_pretrained(\"redrussianarmy/gpt2-turkish-cased\", add_cross_attention=True).to(DEVICE)\ndecoder_model.resize_token_embeddings(len(decoder_tokenizer))\ndecoder_model.config.add_cross_attention = True\n\nmodel = EncoderDecoderModel(encoder=encoder_model, decoder=decoder_model) \noptimizer = optim.Adadelta(model.parameters())\ncriterion = nn.CrossEntropyLoss()",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "id": "5iey1hlJ14Tq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "72750b2d-a55e-45e9-d5ca-539a4fba29ed",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-03-19T18:43:17.696806Z",
     "iopub.execute_input": "2023-03-19T18:43:17.698001Z",
     "iopub.status.idle": "2023-03-19T18:43:40.803440Z",
     "shell.execute_reply.started": "2023-03-19T18:43:17.697963Z",
     "shell.execute_reply": "2023-03-19T18:43:40.802381Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a1ef7e8e004461d9ca42d65871a32b3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9ff8012ef0e4c088f810865bb1a6cf4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57edaa07667443e7a19aff2c6ed445fe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/595 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c4d881dd84e4f5e9c1cb381fb176a87"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0b9474bf192417bb3d9eac655208f8f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d39adaf0a574536a8e5e9e40987b96e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/594k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bed67e031402427582529a82ef3459a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88a486f38b02421597cdf9a24b936495"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/445M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac3dc8f20c10445e8fcdc26551dbd1da"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of the model checkpoint at dbmdz/bert-base-turkish-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/510M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d868ae85bbb44b4493b7a4d87b3826e3"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at redrussianarmy/gpt2-turkish-cased and are newly initialized: ['transformer.h.0.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.5.crossattention.bias', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.10.crossattention.bias', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.0.crossattention.bias', 'transformer.h.9.crossattention.bias', 'transformer.h.3.crossattention.masked_bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.7.crossattention.bias', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.6.crossattention.masked_bias', 'transformer.h.8.crossattention.bias', 'transformer.h.11.crossattention.bias', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.masked_bias', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.11.crossattention.masked_bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.8.crossattention.masked_bias', 'transformer.h.6.crossattention.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.3.crossattention.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.10.crossattention.masked_bias', 'transformer.h.4.crossattention.bias', 'transformer.h.2.crossattention.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.7.crossattention.masked_bias', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.9.ln_cross_attn.weight', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.10.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "for name, param in model.named_parameters():\n    if 'decoder' not in name:\n        param.requires_grad = False",
   "metadata": {
    "id": "YH2s3vh33lCG",
    "execution": {
     "iopub.status.busy": "2023-03-19T19:10:15.502297Z",
     "iopub.execute_input": "2023-03-19T19:10:15.502666Z",
     "iopub.status.idle": "2023-03-19T19:10:15.509998Z",
     "shell.execute_reply.started": "2023-03-19T19:10:15.502634Z",
     "shell.execute_reply": "2023-03-19T19:10:15.508901Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "for name, param in model.named_parameters():\n    param.requires_grad =True",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-19T19:36:23.569650Z",
     "iopub.execute_input": "2023-03-19T19:36:23.570121Z",
     "iopub.status.idle": "2023-03-19T19:36:23.579507Z",
     "shell.execute_reply.started": "2023-03-19T19:36:23.570082Z",
     "shell.execute_reply": "2023-03-19T19:36:23.578131Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_data = TData(train_df, encoder_tokenizer, decoder_tokenizer, device=DEVICE)\nval_data = TData(val_df, encoder_tokenizer, decoder_tokenizer, device=DEVICE)",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "id": "R9v2Vmpa14Tr",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-03-19T18:43:40.817482Z",
     "iopub.execute_input": "2023-03-19T18:43:40.818234Z",
     "iopub.status.idle": "2023-03-19T18:43:41.052378Z",
     "shell.execute_reply.started": "2023-03-19T18:43:40.818197Z",
     "shell.execute_reply": "2023-03-19T18:43:41.051360Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=BATCH_SIZE)",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "id": "d29O2WhO14Tr",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-03-19T18:43:41.056619Z",
     "iopub.execute_input": "2023-03-19T18:43:41.057423Z",
     "iopub.status.idle": "2023-03-19T18:43:41.064805Z",
     "shell.execute_reply.started": "2023-03-19T18:43:41.057385Z",
     "shell.execute_reply": "2023-03-19T18:43:41.063821Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "ex = val_df.iloc[0]\nprint(f'answer -> {ex.answer}\\ncloze -> {ex.cloze}\\nquestion -> {ex.question}')",
   "metadata": {
    "id": "9YPsBOEL14Tt",
    "outputId": "daaa5978-1544-4a89-cd53-50ff1187372c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "execution": {
     "iopub.status.busy": "2023-03-19T18:43:41.065995Z",
     "iopub.execute_input": "2023-03-19T18:43:41.066260Z",
     "iopub.status.idle": "2023-03-19T18:43:41.085391Z",
     "shell.execute_reply.started": "2023-03-19T18:43:41.066236Z",
     "shell.execute_reply": "2023-03-19T18:43:41.083882Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "answer -> 136\ncloze ->  Panthers hattında ayrıca, sadece 9 başlangıçta 5 sack eden uç çizgi savunmacısı Kony Ealy ile birlikte 136 kez ile NFL'nin aktif kariyer sack lideri ve 5 kez profesyonel bir top atıcısı olan Jared Allen öne çıkmaktadır.\nquestion -> Jared Allen'ın kaç tane kariyer sack edişi vardır?\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "train_losses, train_ppls, train_bleus, val_losses, val_ppls, val_bleus = train(model, train_loader, criterion, optimizer, val_loader=val_loader, device=DEVICE)",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    },
    "id": "2hgnXJWI14Tt",
    "outputId": "d9aa8ac6-3ce7-40e1-f91f-955ee6ce36d0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839
    },
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-03-19T19:36:28.875431Z",
     "iopub.execute_input": "2023-03-19T19:36:28.875795Z",
     "iopub.status.idle": "2023-03-19T19:57:57.602680Z",
     "shell.execute_reply.started": "2023-03-19T19:36:28.875763Z",
     "shell.execute_reply": "2023-03-19T19:57:57.600918Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/10\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Batch 566/566: Loss: 0.6356 - Perplexity: 324179258968372.5625 - Bleu: 1.5252: 100%|██████████| 566/566 [03:19<00:00,  2.84it/s] \nBatch 100/100: Loss: 0.3915 - Perplexity: 1.4866 - Bleu: 2.2747: 100%|██████████| 100/100 [00:13<00:00,  7.54it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "GENERATED -> ['<BOS>KKK anlama anlama anlama anlama yapmıştır?<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>']\nEpoch 2/10\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Batch 566/566: Loss: 0.3696 - Perplexity: 1.4540 - Bleu: 2.5621: 100%|██████████| 566/566 [03:19<00:00,  2.84it/s]\nBatch 100/100: Loss: 0.3541 - Perplexity: 1.4312 - Bleu: 3.1969: 100%|██████████| 100/100 [00:13<00:00,  7.59it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "GENERATED -> [\"<BOS>Los'ın yılda yılda yılda yılda yılda yılda kaç kaç kaç kaç kaç kaç kaç sürdü?<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\"]\nEpoch 3/10\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Batch 566/566: Loss: 0.3338 - Perplexity: 1.4022 - Bleu: 3.1967: 100%|██████████| 566/566 [03:19<00:00,  2.84it/s]\nBatch 100/100: Loss: 0.3307 - Perplexity: 1.3976 - Bleu: 3.4835: 100%|██████████| 100/100 [00:13<00:00,  7.57it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "GENERATED -> [\"<BOS>Publ'nin ilk ilk son ilk ilk son adı nedir?<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\"]\nEpoch 4/10\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Batch 566/566: Loss: 0.2937 - Perplexity: 1.3459 - Bleu: 3.7305: 100%|██████████| 566/566 [03:19<00:00,  2.83it/s]\nBatch 100/100: Loss: 0.2946 - Perplexity: 1.3479 - Bleu: 3.5558: 100%|██████████| 100/100 [00:13<00:00,  7.62it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "GENERATED -> [\"<BOS>DEC'in ardından ne olarak adlandırılır?<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\"]\nEpoch 5/10\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Batch 566/566: Loss: 0.2454 - Perplexity: 1.2814 - Bleu: 4.2281: 100%|██████████| 566/566 [03:18<00:00,  2.84it/s]\nBatch 100/100: Loss: 0.2674 - Perplexity: 1.3119 - Bleu: 4.3089: 100%|██████████| 100/100 [00:13<00:00,  7.62it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "GENERATED -> [\"<BOS>Temuçin'in karısı kimdir?<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\"]\nEpoch 6/10\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Batch 566/566: Loss: 0.1974 - Perplexity: 1.2202 - Bleu: 6.0859: 100%|██████████| 566/566 [03:19<00:00,  2.84it/s]\nBatch 100/100: Loss: 0.2629 - Perplexity: 1.3072 - Bleu: 5.1362: 100%|██████████| 100/100 [00:13<00:00,  7.59it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "GENERATED -> ['<BOS>Fransız Protestanları ne anlama gelmektedir?<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>']\nEpoch 7/10\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Batch 14/566: Loss: 0.1230 - Perplexity: 1.1315 - Bleu: 17.5414:   2%|▏         | 14/566 [00:04<03:15,  2.82it/s]\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_23/2549087499.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_losses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_ppls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_bleus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_losses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_ppls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_bleus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mval_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_23/3799086404.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, criterion, optimizer, val_loader, device)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Epoch {epoch+1}/{EPOCHS}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m         \u001B[0mtrain_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_ppl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_bleu\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0mtrain_losses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m         \u001B[0mtrain_ppls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ppl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_23/3799086404.py\u001B[0m in \u001B[0;36mtrain_step\u001B[0;34m(model, train_loader, criterion, optimizer)\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0mh_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mencoder_inputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlast_hidden_state\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m         \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mdecoder_outputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoder_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mh_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoder_attention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoder_inputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'attention_mask'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogits\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmoveaxis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecoder_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'input_ids'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1188\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1191\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1192\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1054\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1055\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1056\u001B[0;31m             \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1057\u001B[0m         )\n\u001B[1;32m   1058\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformer_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1188\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1191\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1192\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    893\u001B[0m                     \u001B[0mencoder_attention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoder_attention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    894\u001B[0m                     \u001B[0muse_cache\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_cache\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 895\u001B[0;31m                     \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    896\u001B[0m                 )\n\u001B[1;32m    897\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1188\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1191\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1192\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    392\u001B[0m             \u001B[0mhead_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhead_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    393\u001B[0m             \u001B[0muse_cache\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_cache\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 394\u001B[0;31m             \u001B[0moutput_attentions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    395\u001B[0m         )\n\u001B[1;32m    396\u001B[0m         \u001B[0mattn_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mattn_outputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m  \u001B[0;31m# output_attn: a, present, (attentions)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1188\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1191\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1192\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    327\u001B[0m             \u001B[0mattn_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_upcast_and_reordered_attn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mquery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhead_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 329\u001B[0;31m             \u001B[0mattn_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_attn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mquery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhead_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    330\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    331\u001B[0m         \u001B[0mattn_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_merge_heads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn_output\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_heads\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001B[0m in \u001B[0;36m_attn\u001B[0;34m(self, query, key, value, attention_mask, head_mask)\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0;31m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m             \u001B[0mmask_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfull\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask_value\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattn_weights\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn_weights\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m             \u001B[0mattn_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcausal_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattn_weights\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattn_weights\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask_value\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    201\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mattention_mask\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "ask(model, ex.answer, ex.cloze, device=DEVICE)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-19T19:09:54.600395Z",
     "iopub.execute_input": "2023-03-19T19:09:54.600759Z",
     "iopub.status.idle": "2023-03-19T19:09:55.753463Z",
     "shell.execute_reply.started": "2023-03-19T19:09:54.600728Z",
     "shell.execute_reply": "2023-03-19T19:09:55.752531Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['<BOS>Broncos Panthers, panthersleri nasıl kullanmıştır?<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "model.save_pretrained('best_bert_gpt2')",
   "metadata": {
    "id": "oUp2Ioq3zGDt",
    "execution": {
     "iopub.status.busy": "2023-03-19T15:12:49.537713Z",
     "iopub.execute_input": "2023-03-19T15:12:49.538456Z",
     "iopub.status.idle": "2023-03-19T15:12:51.353473Z",
     "shell.execute_reply.started": "2023-03-19T15:12:49.538415Z",
     "shell.execute_reply": "2023-03-19T15:12:51.352396Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!zip -r best_bert_gpt2.zip best_bert_gpt2/",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-19T15:15:48.456792Z",
     "iopub.execute_input": "2023-03-19T15:15:48.457707Z",
     "iopub.status.idle": "2023-03-19T15:16:45.037973Z",
     "shell.execute_reply.started": "2023-03-19T15:15:48.457655Z",
     "shell.execute_reply": "2023-03-19T15:16:45.036745Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n  adding: best_bert_gpt2/ (stored 0%)\n  adding: best_bert_gpt2/config.json (deflated 76%)\n  adding: best_bert_gpt2/pytorch_model.bin (deflated 9%)\n  adding: best_bert_gpt2/generation_config.json (deflated 24%)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "while True:\n    pass",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "id": "SR9CJTgFOG80",
    "outputId": "9dcebfa4-5799-4d1f-fc05-7a94672e8b6c",
    "execution": {
     "iopub.status.busy": "2023-03-19T15:17:04.541412Z",
     "iopub.execute_input": "2023-03-19T15:17:04.542174Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "!cp /content/attacker_bert_gpt2.pt /content/drive/MyDrive/adversarial-taboo/adversarial-taboo-models",
   "metadata": {
    "id": "defGD0aSPKS3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "h7BLnxVuQAKO"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
