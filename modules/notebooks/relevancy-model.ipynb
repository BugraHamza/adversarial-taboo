{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers.utils.logging import set_verbosity_error\n",
    "\n",
    "from transformers import BertTokenizer, AutoModel, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def get_bert_tokenizer(path, max_len=512):\n",
    "    tokenizer = BertTokenizer.from_pretrained(path, model_max_length=max_len)\n",
    "    return tokenizer\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class ClassifyBERTurk(nn.Module):\n",
    "    def __init__(self, model_name_or_path):\n",
    "        super(ClassifyBERTurk, self).__init__()\n",
    "\n",
    "        self.berturk = AutoModel.from_pretrained(model_name_or_path)\n",
    "        self.fc = nn.Linear(self.berturk.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        bert_out = self.berturk(**inputs).last_hidden_state\n",
    "        bert_last_hidden = bert_out[:, 0, :]\n",
    "\n",
    "        fc_out = self.fc(bert_last_hidden)\n",
    "        sigmoid_out = self.sigmoid(fc_out)\n",
    "        return sigmoid_out\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def get_relevancy_model(model_name_or_path, trainable_llm=False, device='cpu'):\n",
    "    model = ClassifyBERTurk(model_name_or_path).to(device=device)\n",
    "\n",
    "    for params in model.berturk.parameters():\n",
    "        params.requires_grad = trainable_llm\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class RelevancyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.data.iloc[i]\n",
    "        post, resp, y = row['post'], row['response'], row['is_pair']\n",
    "\n",
    "        tokenized_pair = self.tokenizer(post, resp, padding='max_length', truncation='longest_first', return_tensors='pt')\n",
    "        tokenized_pair = {k: v.squeeze().to(self.device) for k, v in tokenized_pair.items()}\n",
    "\n",
    "        return tokenized_pair, torch.tensor(y, dtype=torch.float).to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def get_data(data_name, task_name):\n",
    "    if data_name == 'reddit':\n",
    "        if task_name == 'lm':\n",
    "            train_data = pd.read_parquet('taboo-datasets/reddit-dataset/tr-reddit_train.parquet')\n",
    "            val_data = pd.read_parquet('taboo-datasets/reddit-dataset/tr-reddit_val.parquet')\n",
    "            test_data = pd.read_parquet('taboo-datasets/reddit-dataset/tr-reddit_test.parquet')\n",
    "        elif task_name == 'cls':\n",
    "            train_data = pd.read_parquet('taboo-datasets/reddit-dataset/tr-reddit-pairs_train.parquet')\n",
    "            val_data = pd.read_parquet('taboo-datasets/reddit-dataset/tr-reddit-pairs_val.parquet')\n",
    "            test_data = pd.read_parquet('taboo-datasets/reddit-dataset/tr-reddit-pairs_test.parquet')\n",
    "        else:\n",
    "            raise ValueError('Invalid task name')\n",
    "    elif data_name == 'forum_dh':\n",
    "        if task_name == 'lm':\n",
    "            train_data = pd.read_parquet('taboo-datasets/donanim-haber-dataset/forum_dh_train.parquet')\n",
    "            val_data = pd.read_parquet('taboo-datasets/donanim-haber-dataset/forum_dh_val.parquet')\n",
    "            test_data = pd.read_parquet('taboo-datasets/donanim-haber-dataset/forum_dh_test.parquet')\n",
    "        elif task_name == 'cls':\n",
    "            train_data = pd.read_parquet('taboo-datasets/donanim-haber-dataset/forum_dh-pairs_train.parquet')\n",
    "            val_data = pd.read_parquet('taboo-datasets/donanim-haber-dataset/forum_dh-pairs_val.parquet')\n",
    "            test_data = pd.read_parquet('taboo-datasets/donanim-haber-dataset/forum_dh-pairs_test.parquet')\n",
    "        else:\n",
    "            raise ValueError('Invalid task name')\n",
    "    else:\n",
    "        raise ValueError('Invalid data name')\n",
    "\n",
    "    return train_data, val_data, test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def calc_accuracy(y_pred, y, relevancy_threshold=0.4):\n",
    "    return ((y_pred > relevancy_threshold) == y).sum().item() / len(y)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, scheduler, relevancy_threshold=0.4):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    losses, accuracies = [], []\n",
    "\n",
    "    for batch, y in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(**batch).reshape(len(y))\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(calc_accuracy(y_pred, y, relevancy_threshold=relevancy_threshold))\n",
    "        pbar.set_description(f'Loss: {np.mean(losses):.5f} - Accuracy: {np.mean(accuracies):.5f}')\n",
    "\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "def evaluate(model, val_loader, criterion, relevancy_threshold=0.4):\n",
    "    model.eval()\n",
    "    pbar = tqdm(val_loader)\n",
    "    losses, accuracies = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, y in pbar:\n",
    "            y_pred = model(**batch).reshape(len(y))\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(calc_accuracy(y_pred, y, relevancy_threshold=relevancy_threshold))\n",
    "            pbar.set_description(f'Loss: {np.mean(losses):.5f} - Accuracy: {np.mean(accuracies):.5f}')\n",
    "\n",
    "    return np.mean(accuracies)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def train_val_fn(data_name, model_name, batch_size, learning_rate, num_epochs,\n",
    "                 device='cpu', threshold=0.4, trainable_llm=False):\n",
    "    # load data\n",
    "    train_data, val_data, test_data = get_data(data_name, 'cls')\n",
    "\n",
    "    # create a tokenizer\n",
    "    tokenizer = get_bert_tokenizer(model_name)\n",
    "\n",
    "    train_set = RelevancyDataset(train_data, tokenizer, device=device)\n",
    "    val_set = RelevancyDataset(val_data, tokenizer, device=device)\n",
    "    # test_set = RelevancyDataset(test_data, tokenizer, device=device)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    # test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # create a model\n",
    "    relevancy_model = get_relevancy_model(model_name, trainable_llm=trainable_llm, device=device)\n",
    "\n",
    "    # create an optimizer\n",
    "    optimizer = optim.AdamW(relevancy_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # create a criterion\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # create a learning rate scheduler\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps // 10\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    for _ in range(num_epochs):\n",
    "        train_acc = train(relevancy_model, train_loader, criterion, optimizer, scheduler, relevancy_threshold=threshold)\n",
    "        val_acc = evaluate(relevancy_model, val_loader, criterion, relevancy_threshold=threshold)\n",
    "        print(f'Train Accuracy: {train_acc:.5f} - Val Accuracy: {val_acc:.5f}')\n",
    "\n",
    "        # if val_acc > best_val_acc:\n",
    "        #     torch.save(relevancy_model, f'modules/judge_system/best_fluency_model/relevancy_{batch_size}_{learning_rate}_{num_epochs}_{threshold}_{int(1000*val_acc)}.pt')\n",
    "        #     best_val_acc = val_acc\n",
    "\n",
    "    return relevancy_model, val_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "\n",
    "def objective(trial, data_name, device):\n",
    "    global best_acc\n",
    "\n",
    "    data_name = data_name\n",
    "    model_name = 'dbmdz/bert-base-turkish-cased'\n",
    "    batch_size = trial.suggest_int('batch_size', 1, 32)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 1e-2, log=True)\n",
    "    num_epochs = 1  # trial.suggest_int('num_epochs', 1, 5)\n",
    "    threshold = trial.suggest_float('threshold', 0.1, 0.9)\n",
    "\n",
    "    model, acc = train_val_fn(data_name, model_name, batch_size, learning_rate, num_epochs, device, threshold)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        torch.save(model, f'best_model.pt')\n",
    "        print('='*50)\n",
    "        print('CURRENT BEST MODEL SAVED!')\n",
    "        print('='*50)\n",
    "        print(f'Batch Size: {batch_size} - Learning Rate: {learning_rate} - Num Epochs: {num_epochs} - Threshold: {threshold} - Accuracy: {acc:.5f}')\n",
    "        best_acc = acc\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def tune_hyperparameters(study_name, data_name, num_trials, device):\n",
    "    study = optuna.create_study(study_name=study_name, storage=f'sqlite:///{study_name}.db',\n",
    "                                direction=\"maximize\", load_if_exists=True,\n",
    "                                pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "    study.optimize(lambda x: objective(x, data_name, device), n_trials=num_trials, gc_after_trial=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'taboo-datasets/reddit-dataset/tr-reddit-pairs_train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_val_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreddit\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdbmdz/bert-base-turkish-cased\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainable_llm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[30], line 4\u001B[0m, in \u001B[0;36mtrain_val_fn\u001B[0;34m(data_name, model_name, batch_size, learning_rate, num_epochs, device, threshold, trainable_llm)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_val_fn\u001B[39m(data_name, model_name, batch_size, learning_rate, num_epochs,\n\u001B[1;32m      2\u001B[0m                  device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m, threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.4\u001B[39m, trainable_llm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;66;03m# load data\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m     train_data, val_data, test_data \u001B[38;5;241m=\u001B[39m \u001B[43mget_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcls\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# create a tokenizer\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m get_bert_tokenizer(model_name)\n",
      "Cell \u001B[0;32mIn[28], line 8\u001B[0m, in \u001B[0;36mget_data\u001B[0;34m(data_name, task_name)\u001B[0m\n\u001B[1;32m      6\u001B[0m     test_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtaboo-datasets/reddit-dataset/tr-reddit_test.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m task_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcls\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m----> 8\u001B[0m     train_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtaboo-datasets/reddit-dataset/tr-reddit-pairs_train.parquet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     val_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtaboo-datasets/reddit-dataset/tr-reddit-pairs_val.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     10\u001B[0m     test_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtaboo-datasets/reddit-dataset/tr-reddit-pairs_test.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/turkish-taboo/lib/python3.9/site-packages/pandas/io/parquet.py:503\u001B[0m, in \u001B[0;36mread_parquet\u001B[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001B[39;00m\n\u001B[1;32m    458\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;124;03mDataFrame\u001B[39;00m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    501\u001B[0m impl \u001B[38;5;241m=\u001B[39m get_engine(engine)\n\u001B[0;32m--> 503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/turkish-taboo/lib/python3.9/site-packages/pandas/io/parquet.py:244\u001B[0m, in \u001B[0;36mPyArrowImpl.read\u001B[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001B[0m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    242\u001B[0m     to_pandas_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit_blocks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m--> 244\u001B[0m path_or_handle, handles, kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilesystem\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43m_get_path_or_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfilesystem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    251\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mparquet\u001B[38;5;241m.\u001B[39mread_table(\n\u001B[1;32m    252\u001B[0m         path_or_handle, columns\u001B[38;5;241m=\u001B[39mcolumns, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    253\u001B[0m     )\u001B[38;5;241m.\u001B[39mto_pandas(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mto_pandas_kwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/turkish-taboo/lib/python3.9/site-packages/pandas/io/parquet.py:102\u001B[0m, in \u001B[0;36m_get_path_or_handle\u001B[0;34m(path, fs, storage_options, mode, is_dir)\u001B[0m\n\u001B[1;32m     92\u001B[0m handles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m fs\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_dir\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;66;03m# fsspec resources can also point to directories\u001B[39;00m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001B[39;00m\n\u001B[0;32m--> 102\u001B[0m     handles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    105\u001B[0m     fs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    106\u001B[0m     path_or_handle \u001B[38;5;241m=\u001B[39m handles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/turkish-taboo/lib/python3.9/site-packages/pandas/io/common.py:865\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    856\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    857\u001B[0m             handle,\n\u001B[1;32m    858\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    861\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    862\u001B[0m         )\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    864\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m--> 865\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    866\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[1;32m    868\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'taboo-datasets/reddit-dataset/tr-reddit-pairs_train.parquet'"
     ]
    }
   ],
   "source": [
    "model, _ = train_val_fn('reddit', 'dbmdz/bert-base-turkish-cased', batch_size=4, learning_rate=1e-4,\n",
    "                        num_epochs=3, device='cpu', threshold=0.4, trainable_llm=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
