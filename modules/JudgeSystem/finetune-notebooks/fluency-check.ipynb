{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, device='cpu'):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        text = self.tokenize(self.data[i])\n",
    "        tokenized_el = self.tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        tokenized_el = {k: v.squeeze().to(self.device) for k, v in tokenized_el.items()}\n",
    "        return tokenized_el\n",
    "\n",
    "    def tokenize(self, row):\n",
    "        text = row.post + self.tokenizer.pad_token + row.response\n",
    "        text = self.tokenizer.bos_token + text + self.tokenizer.eos_token\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def finetune(model, dataloader, epochs, optimizer, device='cpu'):\n",
    "    losses = []\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader)\n",
    "        pbar.set_description(f\"Epoch #{epoch+1}\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            inputs = batch['input_ids']\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            del inputs\n",
    "            del outputs\n",
    "            \n",
    "            pbar.set_postfix(bce_loss=f\"{sum(losses)/len(losses)}\")\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "def test(model, dataloader, device='cpu'):\n",
    "    loss_sum = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, batch in enumerate(pbar):\n",
    "            inputs = batch['input_ids']\n",
    "            loss = model(inputs, labels=inputs).loss\n",
    "            \n",
    "            loss_sum += loss\n",
    "        \n",
    "    loss_sum = loss_sum / i\n",
    "    perplexity = math.exp(loss_sum)\n",
    "    \n",
    "    return perplexity\n",
    "    \n",
    "def calc_perplexity(model, tokenizer, sentence):\n",
    "    sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "    sent = tokenizer(sent, padding='max_length', return_tensors='pt')\n",
    "    sent = sent['input_ids']\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = model(sent, labels=sent).loss\n",
    "        perplexity = math.exp(loss/len(sent))\n",
    "        \n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "DATA_PATH = '../../../datasets/Turkish-Reddit-Dataset/tr-reddit.parquet'\n",
    "MAX_LEN = 512\n",
    "MODEL_PATH = 'redrussianarmy/gpt2-turkish-cased'\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "THRESHOLD = 35\n",
    "EPOCHS = 1\n",
    "LR = 0.0001\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN)\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>', 'sep_token': '<SEP>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "df = df[['post', 'response']].applymap(lambda x: re.sub('\\s', ' ', x))\n",
    "df = df[['post', 'response']].applymap(lambda x: re.sub('View Poll', '', x))\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "train_set = CustomDataset(X_train, tokenizer=tokenizer, device=DEVICE)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_set = CustomDataset(X_test, tokenizer=tokenizer, device=DEVICE)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "gpt_config = AutoConfig.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN, max_position_embeddings=MAX_LEN)\n",
    "gpt_model = AutoModelForCausalLM.from_config(gpt_config)\n",
    "gpt_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "opt = optim.AdamW(gpt_model.parameters(), lr=LR)\n",
    "gpt_model, losses = finetune(gpt_model, train_loader, epochs=EPOCHS, optimizer=opt, device=DEVICE)\n",
    "\n",
    "perplexity = test(gpt_model, test_loader, device=DEVICE)\n",
    "print(f\"Test Set Perplexity: {perplexity}\")\n",
    "\n",
    "torch.save(gpt_model, f'fluency_model_{perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/510M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d3dea195578470a9c594a60bafadf52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PERPLEXITY TRIAL\n",
    "gpt_model = torch.load('fluency_model_3.2621438768707276', map_location=DEVICE)\n",
    "gpt_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   0%|          | 1/75427 [01:42<2146:24:48, 102.45s/it, bce_loss=11.081254959106445]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m opt \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdamW(gpt_model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLR)\n\u001B[0;32m----> 2\u001B[0m gpt_model, losses \u001B[38;5;241m=\u001B[39m \u001B[43mfinetune\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgpt_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[41], line 15\u001B[0m, in \u001B[0;36mfinetune\u001B[0;34m(model, dataloader, epochs, optimizer, device)\u001B[0m\n\u001B[1;32m     13\u001B[0m         losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     14\u001B[0m         loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 15\u001B[0m         \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m         pbar\u001B[38;5;241m.\u001B[39mset_postfix(bce_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28msum\u001B[39m(losses)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(losses)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model, losses\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/torch/optim/adamw.py:162\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    158\u001B[0m             max_exp_avg_sqs\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_exp_avg_sq\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    160\u001B[0m         state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m--> 162\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m          \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m          \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m          \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m          \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m          \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m          \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m          \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m          \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m          \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m          \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/torch/optim/adamw.py:219\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 219\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    228\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/torch/optim/adamw.py:273\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001B[0m\n\u001B[1;32m    270\u001B[0m param\u001B[38;5;241m.\u001B[39mmul_(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m lr \u001B[38;5;241m*\u001B[39m weight_decay)\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m--> 273\u001B[0m \u001B[43mexp_avg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39madd_(grad, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[1;32m    274\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "sent1 = \"Bu ben akÅŸam people kalmak kalÄ±ypor kalacak\"\n",
    "# sent2 = \"Bu konu hakkÄ±ndaki fikirlerinizi de Ã¶ÄŸrenmek isterim.\"\n",
    "ans1 = None\n",
    "\n",
    "sep_token = tokenizer.special_tokens_map['sep_token']\n",
    "sentence = ' '.join([sent1, sep_token, ans1]) if ans1 else sent1\n",
    "\n",
    "sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "sent = tokenizer(sent, return_tensors='pt', truncation=True, padding=True)\n",
    "sent = sent['input_ids'][0].to(device=DEVICE)\n",
    "\n",
    "nll_list = []\n",
    "for i in range(1, len(sent)):\n",
    "    non_len = len(sent) - i\n",
    "    \n",
    "    curr_sent = sent.clone()\n",
    "    curr_sent[:-non_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = gpt_model(sent, labels=curr_sent).loss\n",
    "        nll_list.append(loss*non_len)\n",
    "        \n",
    "autoreg_ppl = torch.exp(torch.stack(nll_list).sum() / len(sent)).item()\n",
    "loss_ppl = torch.exp(gpt_model(sent, labels=sent).loss / len(sent)).item()\n",
    "\n",
    "loss_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2602)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"Bu ben akÅŸam people\"\n",
    "sent1 = \"Bu konu hakkÄ±ndaki fikirlerinizi de Ã¶ÄŸrenmek isterim.\"\n",
    "ans1 = None\n",
    "\n",
    "sep_token = tokenizer.special_tokens_map['sep_token']\n",
    "sentence = ' '.join([sent1, sep_token, ans1]) if ans1 else sent1\n",
    "\n",
    "sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "sent = tokenizer(sent, return_tensors='pt', truncation=True, padding=True)\n",
    "sent = sent['input_ids'][0].to(device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ppl = torch.exp(gpt_model(sent, labels=sent).loss / len(sent))\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.logits.reshape(1, 50261, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
