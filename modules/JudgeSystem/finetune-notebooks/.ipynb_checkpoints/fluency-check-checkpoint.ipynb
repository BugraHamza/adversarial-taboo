{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, tokenizer, device='cpu'):\n",
    "        self.X = X\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        tokenized_el = self.tokenizer(self.X[i], padding='max_length', return_tensors='pt')\n",
    "        \n",
    "        for k, v in tokenized_el.items():\n",
    "            v_device = v.to(self.device)               \n",
    "            tokenized_el[k] = v_device.squeeze()\n",
    "        \n",
    "        return tokenized_el\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def finetune(model, dataloader, epochs, optimizer, device='cpu'):\n",
    "    losses = []\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader)\n",
    "        pbar.set_description(f\"Epoch #{epoch+1}\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            inputs = batch['input_ids']\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            del inputs\n",
    "            del outputs\n",
    "            \n",
    "            pbar.set_postfix(bce_loss=f\"{sum(losses)/len(losses)}\")\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "def test(model, dataloader, device='cpu'):\n",
    "    loss_sum = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, batch in enumerate(pbar):\n",
    "            inputs = batch['input_ids']\n",
    "            loss = model(inputs, labels=inputs).loss\n",
    "            \n",
    "            loss_sum += loss\n",
    "        \n",
    "    loss_sum = loss_sum / i\n",
    "    perplexity = math.exp(loss_sum)\n",
    "    \n",
    "    return perplexity\n",
    "    \n",
    "def calc_perplexity(model, tokenizer, sentence):\n",
    "    sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "    sent = tokenizer(sent, padding='max_length', return_tensors='pt')\n",
    "    sent = sent['input_ids']\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = model(sent, labels=sent).loss\n",
    "        perplexity = math.exp(loss/len(sent))\n",
    "        \n",
    "        return perplexity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokenizable(df, tokenizer):\n",
    "    ls = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        el_sent = ' '.join([row.post, row.response])\n",
    "        el_sent = tokenizer.bos_token + el_sent + tokenizer.eos_token\n",
    "        tok_sent = tokenizer.tokenize(el_sent)\n",
    "                \n",
    "        if len(tok_sent) < tokenizer.model_max_length-1:\n",
    "            ls.append(i)\n",
    "            \n",
    "    return df.iloc[ls, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_tokenizable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     16\u001B[0m df \u001B[38;5;241m=\u001B[39m df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mapplymap(\u001B[38;5;28;01mlambda\u001B[39;00m x: re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m, x))\n\u001B[1;32m     17\u001B[0m df \u001B[38;5;241m=\u001B[39m df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mapplymap(\u001B[38;5;28;01mlambda\u001B[39;00m x: re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mView Poll\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, x))\n\u001B[0;32m---> 19\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mget_tokenizable\u001B[49m(df, tokenizer)\n\u001B[1;32m     21\u001B[0m X \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_numpy(), df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_numpy())\n\u001B[1;32m     22\u001B[0m X_train, X_test \u001B[38;5;241m=\u001B[39m train_test_split(X, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'get_tokenizable' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '/Users/bugrahamzagundog/Desktop/AutoTaboo Player/datasets/Turkish-Reddit-Dataset/tr_reddit-dataset.csv'\n",
    "MAX_LEN = 512\n",
    "MODEL_PATH = 'redrussianarmy/gpt2-turkish-cased'\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = 'cpu' if torch.backends.mps.is_available() else 'cpu'\n",
    "THRESHOLD = 35\n",
    "EPOCHS = 1\n",
    "LR = 0.0001\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN)\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>', 'sep_token': '<SEP>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.drop(columns='Unnamed: 0') if 'Unnamed: 0' in df.columns else df\n",
    "df = df[['post', 'response']].applymap(lambda x: re.sub('\\s', ' ', x))\n",
    "df = df[['post', 'response']].applymap(lambda x: re.sub('View Poll', '', x))\n",
    "\n",
    "df = get_tokenizable(df, tokenizer)\n",
    "\n",
    "X = np.append(df['post'].to_numpy(), df['response'].to_numpy())\n",
    "X_train, X_test = train_test_split(X, test_size=0.2)\n",
    "\n",
    "train_set = CustomDataset(X_train, tokenizer=tokenizer, device=DEVICE)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_set = CustomDataset(X_test, tokenizer=tokenizer, device=DEVICE)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "gpt_config = AutoConfig.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN, max_position_embeddings=MAX_LEN)\n",
    "gpt_model = AutoModelForCausalLM.from_config(gpt_config)\n",
    "gpt_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "opt = optim.AdamW(gpt_model.parameters(), lr=LR)\n",
    "gpt_model, losses = finetune(gpt_model, train_loader, epochs=EPOCHS, optimizer=opt, device=DEVICE)\n",
    "\n",
    "perplexity = test(gpt_model, test_loader, device=DEVICE)\n",
    "print(f\"Test Set Perplexity: {perplexity}\")\n",
    "\n",
    "torch.save(gpt_model, f'fluency_model_{perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERPLEXITY TRIAL\n",
    "gpt_model = torch.load('fluency_model_3.2621438768707276', map_location=DEVICE)\n",
    "gpt_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.414367914199829"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"Bu ben akşam people kalmak kalıypor kalacak\"\n",
    "# sent2 = \"Bu konu hakkındaki fikirlerinizi de öğrenmek isterim.\"\n",
    "ans1 = None\n",
    "\n",
    "sep_token = tokenizer.special_tokens_map['sep_token']\n",
    "sentence = ' '.join([sent1, sep_token, ans1]) if ans1 else sent1\n",
    "\n",
    "sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "sent = tokenizer(sent, return_tensors='pt', truncation=True, padding=True)\n",
    "sent = sent['input_ids'][0].to(device=DEVICE)\n",
    "\n",
    "nll_list = []\n",
    "for i in range(1, len(sent)):\n",
    "    non_len = len(sent) - i\n",
    "    \n",
    "    curr_sent = sent.clone()\n",
    "    curr_sent[:-non_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = gpt_model(sent, labels=curr_sent).loss\n",
    "        nll_list.append(loss*non_len)\n",
    "        \n",
    "autoreg_ppl = torch.exp(torch.stack(nll_list).sum() / len(sent)).item()\n",
    "loss_ppl = torch.exp(gpt_model(sent, labels=sent).loss / len(sent)).item()\n",
    "\n",
    "loss_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2602)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"Bu ben akşam people\"\n",
    "sent1 = \"Bu konu hakkındaki fikirlerinizi de öğrenmek isterim.\"\n",
    "ans1 = None\n",
    "\n",
    "sep_token = tokenizer.special_tokens_map['sep_token']\n",
    "sentence = ' '.join([sent1, sep_token, ans1]) if ans1 else sent1\n",
    "\n",
    "sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "sent = tokenizer(sent, return_tensors='pt', truncation=True, padding=True)\n",
    "sent = sent['input_ids'][0].to(device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ppl = torch.exp(gpt_model(sent, labels=sent).loss / len(sent))\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.logits.reshape(1, 50261, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
