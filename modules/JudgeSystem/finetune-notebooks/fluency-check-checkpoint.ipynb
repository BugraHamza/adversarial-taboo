{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, device='cpu'):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        tokenized_el = self.tokenizer(self.data[i], padding='max_length', return_tensors='pt')\n",
    "        tokenized_el = {k: v.squeeze().to(self.device) for k, v in tokenized_el.items()}\n",
    "        return tokenized_el\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def finetune(model, dataloader, epochs, optimizer, device='cpu'):\n",
    "    losses = []\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader)\n",
    "        pbar.set_description(f\"Epoch #{epoch+1}\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            inputs = batch['input_ids']\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            del inputs\n",
    "            del outputs\n",
    "            \n",
    "            pbar.set_postfix(bce_loss=f\"{sum(losses)/len(losses)}\")\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "def test(model, dataloader, device='cpu'):\n",
    "    loss_sum = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, batch in enumerate(pbar):\n",
    "            inputs = batch['input_ids']\n",
    "            loss = model(inputs, labels=inputs).loss\n",
    "            \n",
    "            loss_sum += loss\n",
    "        \n",
    "    loss_sum = loss_sum / i\n",
    "    perplexity = math.exp(loss_sum)\n",
    "    \n",
    "    return perplexity\n",
    "    \n",
    "def calc_perplexity(model, tokenizer, sentence):\n",
    "    sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "    sent = tokenizer(sent, padding='max_length', return_tensors='pt')\n",
    "    sent = sent['input_ids']\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = model(sent, labels=sent).loss\n",
    "        perplexity = math.exp(loss/len(sent))\n",
    "        \n",
    "        return perplexity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokenizable(df, tokenizer):\n",
    "    ls = []\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        el_sent = ' '.join([row.post, row.response])\n",
    "        el_sent = tokenizer.bos_token + el_sent + tokenizer.eos_token\n",
    "        tok_sent = tokenizer.tokenize(el_sent)\n",
    "                \n",
    "        if len(tok_sent) < tokenizer.model_max_length-1:\n",
    "            ls.append(i)\n",
    "            \n",
    "    return df.iloc[ls, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../../../datasets/Turkish-Reddit-Dataset/tr_reddit-dataset.parquet'\n",
    "MAX_LEN = 512\n",
    "MODEL_PATH = 'redrussianarmy/gpt2-turkish-cased'\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "THRESHOLD = 35\n",
    "EPOCHS = 1\n",
    "LR = 0.0001\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN)\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>', 'sep_token': '<SEP>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet file size is 0 bytes",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../../../datasets/Turkish-Reddit-Dataset/tr_reddit-dataset.parquet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m df\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/pandas/io/parquet.py:503\u001B[0m, in \u001B[0;36mread_parquet\u001B[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001B[39;00m\n\u001B[1;32m    458\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;124;03mDataFrame\u001B[39;00m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    501\u001B[0m impl \u001B[38;5;241m=\u001B[39m get_engine(engine)\n\u001B[0;32m--> 503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/pandas/io/parquet.py:251\u001B[0m, in \u001B[0;36mPyArrowImpl.read\u001B[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001B[0m\n\u001B[1;32m    244\u001B[0m path_or_handle, handles, kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilesystem\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m _get_path_or_handle(\n\u001B[1;32m    245\u001B[0m     path,\n\u001B[1;32m    246\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilesystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    247\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m    248\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    249\u001B[0m )\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 251\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto_pandas(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mto_pandas_kwargs)\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    255\u001B[0m         result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39m_as_manager(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:2737\u001B[0m, in \u001B[0;36mread_table\u001B[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties)\u001B[0m\n\u001B[1;32m   2730\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2731\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m keyword is no longer supported with the new \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2732\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatasets-based implementation. Specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2733\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_legacy_dataset=True\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to temporarily recover the old \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2734\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbehaviour.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2735\u001B[0m     )\n\u001B[1;32m   2736\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2737\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43m_ParquetDatasetV2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2738\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2739\u001B[0m \u001B[43m        \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2740\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2741\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartitioning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartitioning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2742\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2743\u001B[0m \u001B[43m        \u001B[49m\u001B[43mread_dictionary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread_dictionary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2744\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffer_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2746\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_prefixes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_prefixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2747\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpre_buffer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2748\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\n\u001B[1;32m   2749\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2750\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m   2751\u001B[0m     \u001B[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001B[39;00m\n\u001B[1;32m   2752\u001B[0m     \u001B[38;5;66;03m# module is not available\u001B[39;00m\n\u001B[1;32m   2753\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m filters \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:2340\u001B[0m, in \u001B[0;36m_ParquetDatasetV2.__init__\u001B[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, **kwargs)\u001B[0m\n\u001B[1;32m   2336\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m single_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2337\u001B[0m     fragment \u001B[38;5;241m=\u001B[39m parquet_format\u001B[38;5;241m.\u001B[39mmake_fragment(single_file, filesystem)\n\u001B[1;32m   2339\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mFileSystemDataset(\n\u001B[0;32m-> 2340\u001B[0m         [fragment], schema\u001B[38;5;241m=\u001B[39mschema \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mfragment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mphysical_schema\u001B[49m,\n\u001B[1;32m   2341\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mparquet_format,\n\u001B[1;32m   2342\u001B[0m         filesystem\u001B[38;5;241m=\u001B[39mfragment\u001B[38;5;241m.\u001B[39mfilesystem\n\u001B[1;32m   2343\u001B[0m     )\n\u001B[1;32m   2344\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   2346\u001B[0m \u001B[38;5;66;03m# check partitioning to enable dictionary encoding\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/pyarrow/_dataset.pyx:870\u001B[0m, in \u001B[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/pyarrow/error.pxi:144\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/taboo/lib/python3.8/site-packages/pyarrow/error.pxi:100\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mArrowInvalid\u001B[0m: Could not open Parquet input source '<Buffer>': Parquet file size is 0 bytes"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../../../datasets/Turkish-Reddit-Dataset/tr_reddit-dataset.parquet')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_tokenizable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     16\u001B[0m df \u001B[38;5;241m=\u001B[39m df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mapplymap(\u001B[38;5;28;01mlambda\u001B[39;00m x: re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m, x))\n\u001B[1;32m     17\u001B[0m df \u001B[38;5;241m=\u001B[39m df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mapplymap(\u001B[38;5;28;01mlambda\u001B[39;00m x: re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mView Poll\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, x))\n\u001B[0;32m---> 19\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mget_tokenizable\u001B[49m(df, tokenizer)\n\u001B[1;32m     21\u001B[0m X \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_numpy(), df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_numpy())\n\u001B[1;32m     22\u001B[0m X_train, X_test \u001B[38;5;241m=\u001B[39m train_test_split(X, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'get_tokenizable' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "df = df.drop(columns='Unnamed: 0') if 'Unnamed: 0' in df.columns else df\n",
    "df = df[['post', 'response']].applymap(lambda x: re.sub('\\s', ' ', x))\n",
    "df = df[['post', 'response']].applymap(lambda x: re.sub('View Poll', '', x))\n",
    "\n",
    "df = get_tokenizable(df, tokenizer)\n",
    "\n",
    "X = np.append(df['post'].to_numpy(), df['response'].to_numpy())\n",
    "X_train, X_test = train_test_split(X, test_size=0.2)\n",
    "\n",
    "train_set = CustomDataset(X_train, tokenizer=tokenizer, device=DEVICE)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_set = CustomDataset(X_test, tokenizer=tokenizer, device=DEVICE)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "gpt_config = AutoConfig.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN, max_position_embeddings=MAX_LEN)\n",
    "gpt_model = AutoModelForCausalLM.from_config(gpt_config)\n",
    "gpt_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "opt = optim.AdamW(gpt_model.parameters(), lr=LR)\n",
    "gpt_model, losses = finetune(gpt_model, train_loader, epochs=EPOCHS, optimizer=opt, device=DEVICE)\n",
    "\n",
    "perplexity = test(gpt_model, test_loader, device=DEVICE)\n",
    "print(f\"Test Set Perplexity: {perplexity}\")\n",
    "\n",
    "torch.save(gpt_model, f'fluency_model_{perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERPLEXITY TRIAL\n",
    "gpt_model = torch.load('fluency_model_3.2621438768707276', map_location=DEVICE)\n",
    "gpt_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.414367914199829"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"Bu ben akşam people kalmak kalıypor kalacak\"\n",
    "# sent2 = \"Bu konu hakkındaki fikirlerinizi de öğrenmek isterim.\"\n",
    "ans1 = None\n",
    "\n",
    "sep_token = tokenizer.special_tokens_map['sep_token']\n",
    "sentence = ' '.join([sent1, sep_token, ans1]) if ans1 else sent1\n",
    "\n",
    "sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "sent = tokenizer(sent, return_tensors='pt', truncation=True, padding=True)\n",
    "sent = sent['input_ids'][0].to(device=DEVICE)\n",
    "\n",
    "nll_list = []\n",
    "for i in range(1, len(sent)):\n",
    "    non_len = len(sent) - i\n",
    "    \n",
    "    curr_sent = sent.clone()\n",
    "    curr_sent[:-non_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = gpt_model(sent, labels=curr_sent).loss\n",
    "        nll_list.append(loss*non_len)\n",
    "        \n",
    "autoreg_ppl = torch.exp(torch.stack(nll_list).sum() / len(sent)).item()\n",
    "loss_ppl = torch.exp(gpt_model(sent, labels=sent).loss / len(sent)).item()\n",
    "\n",
    "loss_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2602)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"Bu ben akşam people\"\n",
    "sent1 = \"Bu konu hakkındaki fikirlerinizi de öğrenmek isterim.\"\n",
    "ans1 = None\n",
    "\n",
    "sep_token = tokenizer.special_tokens_map['sep_token']\n",
    "sentence = ' '.join([sent1, sep_token, ans1]) if ans1 else sent1\n",
    "\n",
    "sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n",
    "sent = tokenizer(sent, return_tensors='pt', truncation=True, padding=True)\n",
    "sent = sent['input_ids'][0].to(device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ppl = torch.exp(gpt_model(sent, labels=sent).loss / len(sent))\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.logits.reshape(1, 50261, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
