{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom datetime import datetime\nimport math\nfrom tqdm import tqdm\n\nfrom transformers import AutoConfig, AutoTokenizer,  AutoModelForCausalLM, get_linear_schedule_with_warmup",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-03-07T10:51:47.524732Z",
     "iopub.execute_input": "2022-03-07T10:51:47.525364Z",
     "iopub.status.idle": "2022-03-07T10:51:47.534897Z",
     "shell.execute_reply.started": "2022-03-07T10:51:47.525320Z",
     "shell.execute_reply": "2022-03-07T10:51:47.534199Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class CustomDataset(Dataset):\n    def __init__(self, X, tokenizer, device='cpu'):\n        self.X = X\n        self.tokenizer = tokenizer\n        self.device = device\n        \n    def __getitem__(self, i):\n        tokenized_el = self.tokenizer(X[i], padding='max_length', return_tensors='pt')\n        \n        for k, v in tokenized_el.items():\n            v_device = v.to(self.device)               \n            tokenized_el[k] = v_device.squeeze()\n        \n        return tokenized_el\n    \n    def __len__(self):\n        return len(self.X)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-07T10:51:49.627728Z",
     "iopub.execute_input": "2022-03-07T10:51:49.628321Z",
     "iopub.status.idle": "2022-03-07T10:51:49.637228Z",
     "shell.execute_reply.started": "2022-03-07T10:51:49.628284Z",
     "shell.execute_reply": "2022-03-07T10:51:49.636534Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def finetune(model, dataloader, epochs, optimizer, device='cpu'):\n    losses = []\n    \n    model = model.to(device)\n    model.train()\n    \n    for epoch in range(epochs):\n        pbar = tqdm(dataloader)\n        pbar.set_description(f\"Epoch #{epoch+1}\")\n        for i, batch in enumerate(pbar):\n            inputs = batch['input_ids']\n            optimizer.zero_grad()\n\n            outputs = model(inputs, labels=inputs)\n            \n            loss = outputs.loss\n            losses.append(loss.item())\n            loss.backward()\n\n            optimizer.step()\n            \n            del inputs\n            del outputs\n            \n            pbar.set_postfix(bce_loss=f\"{sum(losses)/len(losses)}\")\n\n    return model, losses\n\ndef test(model, dataloader, device='cpu'):\n    loss_sum = 0\n    \n    model.eval()\n    with torch.no_grad():\n        pbar = tqdm(dataloader)\n        for i, batch in enumerate(pbar):\n            inputs = batch['input_ids']\n            loss = model(inputs, labels=inputs).loss\n            \n            loss_sum += loss\n        \n    loss_sum = loss_sum / i\n    perplexity = math.exp(loss_sum)\n    \n    return perplexity\n    \ndef calc_perplexity(model, tokenizer, sentence):\n    sent = tokenizer.bos_token + sentence + tokenizer.eos_token\n    sent = tokenizer(sent, padding='max_length', return_tensors='pt')\n    sent = sent['input_ids']\n    \n    model.eval()\n    with torch.no_grad():\n        loss = model(sent, labels=sent).loss\n        perplexity = math.exp(loss)\n        \n        return perplexity\n    ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-07T10:51:50.321690Z",
     "iopub.execute_input": "2022-03-07T10:51:50.321940Z",
     "iopub.status.idle": "2022-03-07T10:51:50.333351Z",
     "shell.execute_reply.started": "2022-03-07T10:51:50.321913Z",
     "shell.execute_reply": "2022-03-07T10:51:50.332364Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_tokenizable(df, tokenizer):\n    ls = []\n    for i, row in tqdm(df.iterrows()):\n        el_sent = ' '.join([row.post, row.response])\n        el_sent = tokenizer.bos_token + el_sent + tokenizer.eos_token\n        tok_sent = tokenizer.tokenize(el_sent)\n        \n        if len(tok_sent) < tokenizer.model_max_length-1:\n            ls.append(i)\n            \n    return df.iloc[ls, :]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-07T10:51:51.488435Z",
     "iopub.execute_input": "2022-03-07T10:51:51.488881Z",
     "iopub.status.idle": "2022-03-07T10:51:51.494662Z",
     "shell.execute_reply.started": "2022-03-07T10:51:51.488845Z",
     "shell.execute_reply": "2022-03-07T10:51:51.493711Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "MAX_LEN = 512\nMODEL_PATH = 'redrussianarmy/gpt2-turkish-cased'\nBATCH_SIZE = 2\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nTHRESHOLD = 35\nEPOCHS = 10\nLR = 0.005\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN)\nspecial_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n\ndf = pd.read_csv('../input/taboo-datasets/forum_dh.csv')\ndf = get_tokenizable(df, tokenizer)\n\nX = np.append(df['post'].to_numpy(), df['response'].to_numpy())\nX_train, X_test = train_test_split(X, test_size=0.2)\n\ntrain_set = CustomDataset(X_train, tokenizer=tokenizer, device=DEVICE)\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n\ntest_set = CustomDataset(X_test, tokenizer=tokenizer, device=DEVICE)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n\ngpt_config = AutoConfig.from_pretrained(MODEL_PATH, model_max_length=MAX_LEN, max_position_embeddings=MAX_LEN)\ngpt_model = AutoModelForCausalLM.from_config(gpt_config)\ngpt_model.resize_token_embeddings(len(tokenizer))\n\nopt = optim.AdamW(gpt_model.parameters(), lr=LR) \ngpt_model, losses = finetune(gpt_model, train_loader, epochs=EPOCHS, optimizer=opt, device=DEVICE)\nperplexity = test(gpt_model, test_loader, device=DEVICE)\nprint(f\"Test Set Perplexity: {perplexity}\")\n\ntorch.save(gpt_model, f'fluency_model_{perplexity}')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-07T10:51:53.158433Z",
     "iopub.execute_input": "2022-03-07T10:51:53.159144Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "torch.save(bert_model, f'fluency_model_{perplexity}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
